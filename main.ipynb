{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.X_t = None\n",
    "        self.y_hat = None\n",
    "        self.df = None\n",
    "        self.w = None\n",
    "    def normalize(self):\n",
    "        mean_x = np.nanmean(X, axis=1)\n",
    "        std_x = np.nanstd(X, axis=1)\n",
    "        std_x[std_x == 0] = 1\n",
    "        self.X = (self.X - mean_x[:, np.newaxis]) / std_x[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Dataset()\n",
    "testset = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.df = pd.read_csv('train_datas_0.csv', dtype='float', na_values='-')\n",
    "trainset.df1 = pd.read_csv('train_datas_1.csv').apply(pd.to_numeric, errors='coerce')\n",
    "testset.df = pd.read_csv('test_datas.csv', dtype='float', na_values='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 15*9 + 3 # with 2nd-order term, hour, day and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_with_mean():\n",
    "    trainset.df1.dropna(how='all')\n",
    "    for s in (trainset.df, trainset.df1, testset.df):\n",
    "        s[s.columns] = s[s.columns].fillna(s[s.columns].mean())\n",
    "fill_nan_with_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_data():\n",
    "    d = DIM\n",
    "    n = len(trainset.df) - 9 #+ len(trainset.df1) - 9\n",
    "    X = np.zeros((d, n))\n",
    "    y_hat = np.zeros(n)\n",
    "\n",
    "    # extract feature\n",
    "    hr = 0\n",
    "    day = 0\n",
    "    for i in range(len(trainset.df)-9):\n",
    "        X[0:15*9, i] = trainset.df.iloc[i:i+9].to_numpy().flatten()\n",
    "        #X[15*9:15*9*2, i] = X[0:15*9, i] ** 2\n",
    "        X[15*9, i] = hr\n",
    "        X[15*9+1, i] = day\n",
    "        X[15*9+2, i] = 1 # bias\n",
    "        hr = (hr + 1) % 24\n",
    "        day = (day+1) % 365\n",
    "        y_hat[i] = trainset.df.iloc[i+9]['PM2.5']\n",
    "#    hr = 0\n",
    "#     off = len(trainset.df)-9\n",
    "#     for i in range(len(trainset.df1)-9):\n",
    "#         X[0:15*9, off+i] = trainset.df1.iloc[i:i+9].to_numpy().flatten()\n",
    "#         X[15*9:15*9*2, off+i] = X[0:15*9, off+i] ** 2\n",
    "#         X[15*9*2, off+i] = hr\n",
    "#         hr = (hr + 1) % 24\n",
    "#         y_hat[i] = trainset.df1.iloc[i+9]['PM2.5']\n",
    "\n",
    "    trainset.X = X\n",
    "    trainset.X_t = X.T\n",
    "    trainset.y_hat = y_hat\n",
    "    #trainset.normalize()\n",
    "\n",
    "preprocess_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('a', trainset.X, fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y_hat, lr, reg, num_iter):\n",
    "    # training\n",
    "    X_t = X.T\n",
    "    d, n = X.shape\n",
    "    w = np.zeros(d)\n",
    "    sum_grad_sq = 0 # adagrad\n",
    "    loss = np.zeros(num_iter)\n",
    "    for i in range(num_iter):\n",
    "        # L(w) = || X^T - y^ ||^2 + reg * ||w||^2\n",
    "        # âˆ‡L(w) = 2 * X * (X^T * w - y^) + 2 * reg * w\n",
    "        y = np.dot(X_t, w)\n",
    "        loss[i] = np.inner(y - y_hat, y - y_hat) + reg * np.linalg.norm(w, 1)\n",
    "        grad = np.dot(X, y - y_hat) + reg * w\n",
    "        sum_grad_sq += np.inner(grad, grad)\n",
    "        w = w - lr / np.sqrt(sum_grad_sq) * grad\n",
    "        if i == num_iter-1:\n",
    "            print('i',i)\n",
    "            #print('std_x',std_x)\n",
    "            #print('rr',rr)\n",
    "            #print('Xt',X_t)\n",
    "            print('y', y)\n",
    "            print('y_hat', y_hat)\n",
    "            #print('grad', grad)\n",
    "            print('w', w)\n",
    "            print('loss', loss[i])\n",
    "            print()\n",
    "    plt.plot(loss)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(X, y_hat, w):\n",
    "    # validate\n",
    "    X_t = X.T\n",
    "    y = np.dot(X_t, w)\n",
    "    return np.inner(y - y_hat, y - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.1 reg 0\n",
      "i 9999\n",
      "y [30.2  27.14 31.33 ... 14.34 13.12 18.9 ]\n",
      "y_hat [26. 27. 24. ... 12. 17. 19.]\n",
      "w [ 0.01 -0.    0.    0.   -0.    0.01 -0.    0.   -0.   -0.02  0.04  0.\n",
      "  0.   -0.   -0.    0.01 -0.    0.    0.01 -0.    0.   -0.    0.   -0.\n",
      " -0.02  0.04 -0.    0.   -0.    0.    0.01 -0.02 -0.01  0.01  0.    0.01\n",
      " -0.    0.   -0.   -0.01  0.06  0.    0.   -0.   -0.01  0.01 -0.   -0.\n",
      "  0.    0.   -0.01  0.    0.   -0.   -0.02  0.04  0.   -0.   -0.   -0.\n",
      "  0.01 -0.01 -0.01  0.    0.   -0.02  0.    0.   -0.   -0.03  0.05  0.\n",
      "  0.   -0.   -0.01  0.01 -0.01 -0.01  0.   -0.   -0.02  0.    0.   -0.\n",
      " -0.03  0.07  0.    0.   -0.   -0.02  0.01 -0.01 -0.02 -0.   -0.   -0.02\n",
      "  0.    0.   -0.   -0.04  0.09 -0.    0.   -0.01 -0.01  0.   -0.03 -0.03\n",
      " -0.   -0.    0.01  0.    0.   -0.   -0.01  0.16 -0.   -0.   -0.01  0.\n",
      "  0.01 -0.01  0.06  0.07  0.    0.04  0.01  0.    0.    0.22  0.35 -0.\n",
      " -0.   -0.01  0.05  0.02  0.    0.  ]\n",
      "loss 244646.89380003154\n",
      "\n",
      "validate loss 64441.11305741157\n",
      "\n",
      "lr 0.05 reg 0\n",
      "i 9999\n",
      "y [30.23 27.24 31.25 ... 14.23 13.13 18.85]\n",
      "y_hat [26. 27. 24. ... 12. 17. 19.]\n",
      "w [ 0.01 -0.   -0.    0.   -0.    0.01 -0.    0.   -0.   -0.02  0.04  0.\n",
      "  0.   -0.   -0.    0.01 -0.    0.    0.01  0.    0.01 -0.    0.   -0.\n",
      " -0.02  0.05 -0.    0.   -0.   -0.    0.01 -0.01 -0.01  0.01  0.    0.01\n",
      " -0.    0.   -0.   -0.01  0.06  0.    0.   -0.   -0.01  0.01 -0.   -0.\n",
      "  0.    0.   -0.01  0.    0.   -0.   -0.03  0.05  0.   -0.   -0.   -0.\n",
      "  0.01 -0.01 -0.    0.    0.   -0.02  0.    0.   -0.   -0.03  0.06  0.\n",
      "  0.   -0.   -0.01  0.01 -0.01 -0.01  0.   -0.   -0.02  0.    0.   -0.\n",
      " -0.03  0.07  0.    0.   -0.   -0.02  0.01 -0.02 -0.02 -0.   -0.   -0.02\n",
      "  0.    0.   -0.   -0.04  0.09 -0.    0.   -0.01 -0.01  0.   -0.03 -0.03\n",
      " -0.   -0.    0.01  0.    0.   -0.   -0.    0.16 -0.   -0.   -0.01  0.\n",
      "  0.01 -0.    0.06  0.07  0.    0.04  0.    0.    0.    0.21  0.33 -0.\n",
      " -0.   -0.01  0.04  0.02  0.    0.  ]\n",
      "loss 246300.22829127283\n",
      "\n",
      "validate loss 64920.291011051355\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run():\n",
    "    X = trainset.X\n",
    "    y_hat = trainset.y_hat\n",
    "    d, n = X.shape\n",
    "    idx_validate = pd.Series([False] * n)\n",
    "    idx_validate[0:1000] = True\n",
    "    \n",
    "    lr = 1e-1 # learning rate\n",
    "    reg = 1e-3 # regularization term, lasso or ridge\n",
    "    \n",
    "    for lr in (0.1, 0.05):\n",
    "        for reg in (0,):\n",
    "            print('lr',lr,'reg',reg)\n",
    "            testset.w = train(X[:, ~idx_validate], y_hat[~idx_validate], lr, reg, 10000)\n",
    "            loss = validate(X[:, idx_validate], y_hat[idx_validate], testset.w)\n",
    "            print('validate loss', loss)\n",
    "            print()\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.7 ],\n",
       "       [15.15],\n",
       "       [ 2.56],\n",
       "       ...,\n",
       "       [ 4.5 ],\n",
       "       [-1.53],\n",
       "       [ 2.35]])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(threshold=1e3, suppress=True, precision=2)\n",
    "d,n=trainset.X.shape\n",
    "#np.concatenate((trainset.X.T.dot(testset.w), trainset.y_hat)).reshape((n,2))\n",
    "np.vstack(trainset.X.T.dot(testset.w) - trainset.y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_testing_data():\n",
    "    d = DIM\n",
    "    n = (len(testset.df) + 8) // 9\n",
    "    X = np.zeros((d, n))\n",
    "    X[d-1, :] = 1 # for bias\n",
    "\n",
    "    # extract feature\n",
    "    hr = 0\n",
    "    for i in range(n):\n",
    "        X[0:15*9, i] = testset.df.iloc[i*9:(i+1)*9].to_numpy().flatten()\n",
    "        #X[15*9:15*9*2, i] = X[0:15*9, i] ** 2\n",
    "        X[15*9, i] = hr\n",
    "        hr = (hr +1 )%24\n",
    "\n",
    "    testset.X = X\n",
    "    testset.X_t = X.T\n",
    "\n",
    "    #testset.normalize()\n",
    "\n",
    "preprocess_testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 17, 19, 33, 22, 35, 46, 65, 71, 54, 47, 47, 60, 52, 40, 45, 33,\n",
       "       26, 24, 33, 33, 30, 27, 23, 24, 16,  7,  9, 20, 21, 26, 18, 20, 17,\n",
       "       23, 25, 19, 19,  8,  7, 14, 25, 22, 25, 20, 29, 32, 33, 34, 31, 45,\n",
       "       34, 37, 39, 22, 23, 17, 15,  6,  4,  3,  0,  7,  8, 12,  7,  5,  9,\n",
       "       21, 22, 28, 30, 24, 20, 49, 24, 21, 16, 17, 16, 16, 14,  8, 11, 19,\n",
       "       24, 10,  8,  8,  8, 14, 12, 16, 20, 26, 30, 38, 35, 27, 22, 51, 34,\n",
       "       24, 11, -7, 28, 28, 19, 15,  5,  4, 13, 18, 14, 15, 20, 23, 23, 25,\n",
       "       21, 20, 25, 26, 29, 23, 27, 25, 23, 40, 52, 38, 11, 12,  8, 23, 17,\n",
       "       16, 20, 26, 17, 29, 31, 26, 13, 12, 16, 21, 27, 27, 19,  2,  8, 10,\n",
       "       21, 24, 25,  6, 15, 25, 26, 24, 24, 13, 10,  6, 16, 29, 32, 32, 28,\n",
       "       37, 18,  3, 14, 15, 20, 24, 21, 29, 27, 23, 27, 27, 36, 29, 29, 20,\n",
       "       27, 33, 32, 25, 26, 25, 19, 21, 23, 20, 28, 22, 29, 37, 24, 23, 23,\n",
       "       18, 11, 10,  6,  4, 17, 18, 18, 15, 16, 22, 21, 14, 16, 19, 24, 23,\n",
       "       30, 20, 17, 21, 19, 24, 18,  0, 19, 17, 17, 30, 26, 25, 20, 31, 32,\n",
       "       34, 27, 33, 19,  9, 12, 20, 30, 33, 25, 46, 44, 30, 28, 39, 25, 28,\n",
       "       33, 35, 23, 24, 19, 16, 25, -1, 29, 30, 19, 12, 16, 17, 21, 17, 23,\n",
       "       22, 38, 35, 34, 22, 23, 29, 38, 33, 31, 33, 35, 39, 38, 35, 35, 21,\n",
       "       19, 22, 16, 17,  7, 10, 13, 11, 14, 11,  6,  9,  8,  9, 12, 13,  8,\n",
       "        5,  9,  5,  5,  8,  3, 14, 21, 21, 16, 15, 22, 20, 22, 14, 19, 18,\n",
       "       17,  9,  3,  4,  8,  9,  8,  6,  3,  8,  6, 14, 14, 17,  6,  9, 11,\n",
       "        7, 12, 16, 10,  2, 11,  9, 12,  0,  4,  5,  9,  7, 15,  7, 15,  8,\n",
       "        7,  8,  6,  5,  7,  6,  1,  8,  3,  7,  6,  6,  3,  5,  8,  4, 10,\n",
       "        5,  5,  6,  4,  1,  6,  4,  4, 12,  7, 10, 12, 10,  8,  7,  8, 10,\n",
       "        4,  4,  5,  2, 10, 10,  9,  9,  6,  8, 12,  6, 12,  6,  7,  7,  8,\n",
       "       20,  7,  5,  4,  8,  6,  6, 13,  5,  7,  4,  3,  7,  6,  7,  9,  7,\n",
       "       11, 10, 11,  5,  9,  2,  4,  8,  7,  7,  6,  9, 10,  9,  9, 10, 10,\n",
       "        9, 13,  9, 12,  7,  6,  4,  6,  9,  7, 15,  2,  5,  6, 11,  7,  5,\n",
       "       11, 14, 10, 10,  5, 11,  1, 12,  5,  6,  8,  3,  3, 10,  8,  4,  4,\n",
       "        7,  3,  5,  6,  8,  8, 10, 13,  4,  9,  7,  6,  2,  5, 10,  6,  4,\n",
       "        8,  8, 10,  6,  8, 14, 10])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict():\n",
    "    y_pred = np.dot(testset.X_t, testset.w).round(0).astype(int)\n",
    "    return y_pred\n",
    "y_pred = predict()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "    'id': ['id_' + str(i) for i in range(500)],\n",
    "    'value': y_pred\n",
    "})\n",
    "pred_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat [[ 0.13  2.18 -2.18  2.19 -0.48  0.   -0.02 -0.62 -0.41]\n",
      " [-0.01  0.01 -0.23  0.    0.33  0.04 -0.06 -2.43  2.48]\n",
      " [-2.5  -0.7  -0.    0.17 -0.9  -0.41 -0.01  0.03 -0.14]\n",
      " [ 0.   -0.43 -0.04  0.12 -1.52  1.44 -1.44  1.6   0.03]\n",
      " [-1.77  1.33  1.41  0.01  0.04 -0.28  0.    0.15  0.  ]\n",
      " [ 0.06  0.9  -0.87  0.85 -0.62  0.01  0.76  0.28 -0.55]\n",
      " [-0.   -0.01  0.14 -0.   -0.39 -0.05 -0.13 -0.94  0.92]\n",
      " [-0.91  0.11 -0.01  0.34 -1.99 -0.35 -0.02  0.04 -0.3 ]\n",
      " [ 0.    0.3   0.09  0.09 -2.59  2.56 -2.54 -0.48 -0.03]\n",
      " [ 0.53 -0.87 -0.79  0.    0.03  0.69  0.    0.27 -0.07]\n",
      " [ 0.11 -1.26  1.24 -1.24  0.34 -0.01  0.63 -0.68 -0.06]\n",
      " [-0.07  0.08  0.27  0.   -0.27  0.04 -0.27 -0.87  0.87]\n",
      " [-1.02 -0.23 -0.06 -0.14 -0.74 -0.36 -0.02  0.19 -0.01]\n",
      " [-0.   -0.69 -0.3   0.12  0.11 -0.08  0.24  0.6   0.09]\n",
      " [ 2.58  0.13 -1.52  0.15  0.49 -1.64 -0.    0.63  0.25]] [0.02 0.   7.4 ]\n",
      "w [[ 0.01 -0.01 -0.    0.   -0.    0.01  0.    0.   -0.  ]\n",
      " [-0.02  0.01 -0.    0.   -0.01  0.01  0.    0.01  0.02]\n",
      " [ 0.   -0.    0.01  0.    0.   -0.   -0.    0.03 -0.  ]\n",
      " [-0.   -0.01  0.01  0.01 -0.02 -0.01  0.02  0.    0.03]\n",
      " [ 0.    0.   -0.    0.01  0.04 -0.    0.   -0.01 -0.  ]\n",
      " [ 0.   -0.01  0.01  0.02 -0.   -0.    0.    0.   -0.  ]\n",
      " [-0.01  0.03 -0.   -0.   -0.01 -0.    0.   -0.02 -0.01]\n",
      " [ 0.02 -0.   -0.03  0.    0.   -0.   -0.02  0.05 -0.  ]\n",
      " [ 0.   -0.01 -0.   -0.   -0.03 -0.02  0.01 -0.   -0.02]\n",
      " [ 0.    0.   -0.   -0.02  0.05 -0.    0.   -0.01 -0.02]\n",
      " [-0.    0.   -0.01 -0.01 -0.   -0.03  0.    0.    0.  ]\n",
      " [-0.07  0.09 -0.    0.   -0.01 -0.01 -0.   -0.01 -0.03]\n",
      " [-0.02 -0.    0.02  0.    0.    0.   -0.01  0.16 -0.  ]\n",
      " [-0.   -0.01 -0.01  0.01  0.01  0.07  0.06  0.    0.03]\n",
      " [ 0.01  0.    0.    0.22  0.38 -0.01 -0.   -0.01  0.05]] [0.03 0.   0.  ]\n",
      "loss 289917.6794856703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soyccan/anaconda3/envs/ml-hw1-tmp/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def closed_form_solution():\n",
    "    w_hat, residuals, rank, s  = np.linalg.lstsq(trainset.X_t, trainset.y_hat)\n",
    "    #print(residuals, rank, s)\n",
    "    X = trainset.X\n",
    "    X_t = trainset.X_t\n",
    "    y_hat = trainset.y_hat\n",
    "    assert w_hat.shape[0] == DIM\n",
    "    print('w_hat',w_hat[:15*9].reshape((15,9)),w_hat[15*9:])\n",
    "    print('w',testset.w[:15*9].reshape((15,9)),testset.w[15*9:])\n",
    "    print('loss', np.linalg.norm(trainset.X_t.dot(w_hat) - trainset.y_hat)**2)\n",
    "closed_form_solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
