{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 15*9 + 3 # with 2nd-order term, hour, day and bias\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.X_t = None\n",
    "        self.y_hat = None\n",
    "        self.df = None\n",
    "        self.w = None\n",
    "    def normalize(self):\n",
    "        mean_x = np.nanmean(self.X, axis=1)\n",
    "        std_x = np.nanstd(self.X, axis=1)\n",
    "        std_x[std_x == 0] = 1\n",
    "        self.X = (self.X - mean_x[:, np.newaxis]) / std_x[:, np.newaxis]\n",
    "        self.X_t = self.X.T\n",
    "        \n",
    "trainset = Dataset()\n",
    "trainset.df = pd.read_csv('train_datas_0.csv', dtype='float', na_values='-')\n",
    "trainset.df1 = pd.read_csv('train_datas_1.csv').apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "testset = Dataset()\n",
    "testset.df = pd.read_csv('test_datas.csv', dtype='float', na_values='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_data():\n",
    "    # correct dataframe\n",
    "    trainset.df1.drop(trainset.df1.index[-2+2162:-2+2208], inplace=True)\n",
    "    trainset.df1.dropna(how='all', inplace=True)\n",
    "    trainset.df1.drop(trainset.df1.index[(trainset.df1 == 0).all(axis=1)], inplace=True)\n",
    "    for s in (trainset.df, trainset.df1, testset.df):\n",
    "        for cnam in s.columns:\n",
    "            # fill NaN with mean\n",
    "            s[cnam].fillna(s[cnam].mean(), inplace=True)\n",
    "\n",
    "            # replace outlier with mean\n",
    "            # TODO: inplace?\n",
    "            s[cnam][abs(s[cnam] - s[cnam].mean()) > 10 * s[cnam].std()] = s[cnam].mean()\n",
    "\n",
    "        s.reindex()\n",
    "\n",
    "    # extract feature\n",
    "    d = DIM\n",
    "    n = len(trainset.df) - 9 + len(trainset.df1) - 9\n",
    "    X = np.zeros((d, n))\n",
    "    y_hat = np.zeros(n)\n",
    "    \n",
    "    for i in range(len(trainset.df)-9):\n",
    "        X[0:15*9, i] = trainset.df.iloc[i:i+9].to_numpy().flatten()\n",
    "        X[15*9, i] = i % 24 # hour\n",
    "        X[15*9+1, i] = (i // 24) % 365 # day\n",
    "        X[15*9+2, i] = 1 # bias\n",
    "        y_hat[i] = trainset.df.iloc[i+9]['PM2.5']\n",
    "    \n",
    "    off = len(trainset.df) - 9\n",
    "    for i in range(len(trainset.df1)-9):\n",
    "        X[0:15*9, i+off] = trainset.df1.iloc[i:i+9].to_numpy().flatten()\n",
    "        X[15*9, i+off] = i % 24 # hour\n",
    "        X[15*9+1, i+off] = (i // 24) % 365 # day\n",
    "        X[15*9+2, i+off] = 1 # bias\n",
    "        y_hat[i] = trainset.df1.iloc[i+9]['PM2.5']\n",
    "\n",
    "    trainset.X = X\n",
    "    trainset.X_t = X.T\n",
    "    trainset.y_hat = y_hat\n",
    "    #trainset.normalize()\n",
    "\n",
    "preprocess_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('a.csv', trainset.X_t, fmt='%.2f', delimiter=',')\n",
    "# c = trainset.df1['SO2']\n",
    "# std_c = c.std()\n",
    "# mean_c = c.mean()\n",
    "# c[abs(c - mean_c) > 3 * std_c]\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "#trainset.X[:,8753-2+2154:8753-2+2300]\n",
    "#trainset.df1.iloc[2534:2550]\n",
    "#trainset.df1[trainset.df1.isnull().any(axis=1)]\n",
    "#trainset.df1.iloc[7518:7800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y_hat, lr, reg, num_iter, b1, b2):\n",
    "    # training\n",
    "    print('lr',lr,'reg',reg,'num_iter',num_iter,'b1',b1,'b2',b2)\n",
    "    \n",
    "    X_t = X.T\n",
    "    d, n = X.shape\n",
    "    w = np.zeros(d)\n",
    "    m = np.zeros(d) # momentum\n",
    "    v = 0 # avg. sq. of past grad.\n",
    "    loss = np.zeros(num_iter)\n",
    "    for i in range(num_iter):\n",
    "        # L(w) = ‖ Xᵀ - ŷ ‖² + reg * ‖w‖²\n",
    "        # ∇L(w) = 2 * X * (Xᵀ * w - ŷ) + 2 * reg * w\n",
    "        y = np.dot(X_t, w)\n",
    "        loss[i] = np.inner(y - y_hat, y - y_hat) + reg * np.linalg.norm(w, 1)\n",
    "        grad = np.dot(X, y - y_hat) + reg * w\n",
    "        \n",
    "        # Adagrad\n",
    "        #v += np.inner(grad, grad)\n",
    "        #w = w - lr / np.sqrt(v) * grad\n",
    "        \n",
    "        # Momentum\n",
    "        m = b1 * m - lr * grad\n",
    "        w = w + m\n",
    "        \n",
    "        # Adam\n",
    "        #m = b1 * m + (1-b1) * grad\n",
    "        #v = b2 * v + (1-b2) * np.inner(grad, grad)\n",
    "        #w = w - lr * (m / (1-b1)) / (np.sqrt(v / (1-b2)) + 1e-10)\n",
    "        \n",
    "        if i == num_iter-1:\n",
    "            #print('i',i)\n",
    "            #print('std_x',std_x)\n",
    "            #print('rr',rr)\n",
    "            #print('Xt',X_t)\n",
    "            print('y', y)\n",
    "            print('y_hat', y_hat)\n",
    "            #print('grad', grad)\n",
    "            print('w', w)\n",
    "            print('loss', loss[i])\n",
    "    plt.plot(loss)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(X, y_hat, w):\n",
    "    # validate\n",
    "    X_t = X.T\n",
    "    y = np.dot(X_t, w)\n",
    "    return np.inner(y - y_hat, y - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1e-10 reg 0 num_iter 1000 b1 0.98 b2 0.9\n",
      "y [23.87 23.2  20.29 ... 11.28  9.57  9.39]\n",
      "y_hat [21. 14. 11. ...  0.  0.  0.]\n",
      "w [ 0.21 -0.01  0.    0.03  0.04  0.   -0.02  0.   -0.02 -0.11  0.18  0.05\n",
      " -0.    0.04  0.01  0.21 -0.01 -0.    0.01  0.04 -0.01 -0.02  0.   -0.02\n",
      " -0.08  0.14  0.05 -0.    0.04 -0.01  0.2  -0.01 -0.    0.02  0.04  0.\n",
      " -0.02  0.   -0.02 -0.07  0.1   0.05 -0.    0.04 -0.    0.2  -0.01 -0.01\n",
      "  0.02  0.03 -0.   -0.02  0.   -0.02 -0.06  0.09  0.06 -0.    0.03 -0.\n",
      "  0.2  -0.01 -0.    0.03  0.03 -0.   -0.02  0.   -0.02 -0.05  0.08  0.06\n",
      " -0.    0.03 -0.01  0.2  -0.02 -0.    0.03  0.03  0.   -0.02  0.   -0.02\n",
      " -0.05  0.08  0.06 -0.    0.02 -0.02  0.2  -0.01  0.    0.03  0.03  0.\n",
      " -0.02  0.   -0.02 -0.06  0.1   0.06 -0.    0.02 -0.01  0.21 -0.01  0.\n",
      "  0.03  0.03  0.   -0.02  0.   -0.02 -0.08  0.12  0.06 -0.    0.02 -0.01\n",
      "  0.21 -0.02  0.02  0.05  0.03  0.02 -0.02  0.   -0.02 -0.08  0.18  0.06\n",
      " -0.    0.01 -0.01  0.06  0.03  0.02]\n",
      "loss 2359821.990162039\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run():\n",
    "    X = trainset.X\n",
    "    y_hat = trainset.y_hat\n",
    "    d, n = X.shape\n",
    "    \n",
    "    lr = 1e-10 # learning rate\n",
    "    reg = 0 # regularization term, lasso or ridge\n",
    "    validate_sz = 2000\n",
    "    num_iter = 1000\n",
    "    b1 = 0.98\n",
    "    b2 = 0.9\n",
    "  \n",
    "    testset.w = train(X, y_hat, lr, reg, num_iter, b1, b2)\n",
    "\n",
    "#     loss = []\n",
    "#     for i in range(0, n, validate_sz):\n",
    "#         idx_validate = pd.Series([False] * n)\n",
    "#         idx_validate[i:i+validate_sz] = True\n",
    "\n",
    "#         testset.w = train(X[:, ~idx_validate], y_hat[~idx_validate], lr, reg, num_iter, b1, b2)\n",
    "#         loss.append(validate(X[:, idx_validate], y_hat[idx_validate], testset.w))\n",
    "\n",
    "#     print('validate loss', np.mean(loss))\n",
    "#     print()\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#np.set_printoptions(threshold=1e3, suppress=True, precision=2)\n",
    "#d,n=trainset.X.shape\n",
    "#np.concatenate((trainset.X.T.dot(testset.w), trainset.y_hat)).reshape((n,2))\n",
    "#np.vstack(trainset.X.T.dot(testset.w) - trainset.y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_testing_data():\n",
    "    d = DIM\n",
    "    n = (len(testset.df) + 8) // 9\n",
    "    X = np.zeros((d, n))\n",
    "\n",
    "    # extract feature\n",
    "    for i in range(n):\n",
    "        X[0:15*9, i] = testset.df.iloc[i*9:(i+1)*9].to_numpy().flatten()\n",
    "        X[15*9, i] = i % 24 # hour\n",
    "        X[15*9+1, i] = (i // 24) % 365 # day\n",
    "        X[15*9+2, i] = 1 # bias\n",
    "    testset.X = X\n",
    "    testset.X_t = X.T\n",
    "\n",
    "    #testset.normalize()\n",
    "\n",
    "preprocess_testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1,   2,  -1,   5,  -3,   1,  13,  11,  15,   7,   5,   3,   6,\n",
       "         7,   3,   9,   5,   2,  -2,   2,  -1,  -3,  -5,  -1,  -2,  -1,\n",
       "        -3,   0,  -1,   0,  -2,  -1,  -2,  -2,  -3,   2,   1,   1,  -5,\n",
       "         0,   0,  -1,   0,   1,   4,   2,   2,   3,   3,   4,   9,   4,\n",
       "         2,   8,   3,   1,   5,  -2,  -3,  -4,  -4,  -2,  -4,  -2,  -2,\n",
       "        -1,  -3,   1,   3,   8,   1,   5,   3,   2,   7,   4,   0,  -2,\n",
       "        -5,  -1,  -1,   0,   0,  -1,  -2,  -2,  -8,  -2,  -3,   1,  -3,\n",
       "         0,   1,   1,  -1,   0,  -3,  -1,  -6,  -3,   9,   5,  -1,  -3,\n",
       "         0,   1,   0,  -3,  -6,  -6,  -3,  -6,   0,   1,  -3,  -3,  -2,\n",
       "        -5,  -1,  -6,  -6,  -8,  -8,   0,   0,   0,  -5, -14,  -6,  -9,\n",
       "        -1,  -4,  -2,  -5,  -2,   1,  -7,  -2,  -7,  -5,   1,  11,   6,\n",
       "         5,   0,   2,   5,   1,   7,  11,  -1,   0,  -3,   0,  -1,  -4,\n",
       "        -2,   4,   2,  -4,  -8, -13,  -6,  -2,   1,   1,   2,  -3,  -2,\n",
       "        -6,   1,  -2,  -5,   1,  -1,  -5,  -5,  -6,   1,  -1,  -3,  -6,\n",
       "        -4,   5,   3,   4,   7,   0, -10,  -3,  -3,  -3,   0,  -4,  -4,\n",
       "         0,  -2,  -4,  -7,  -7,   4,   3,   2,  -1,   0,   0,  -1,   1,\n",
       "        -2,   0,  -6,  -4,   0,   0,   3,   2,   3,   3,   6,  11,  10,\n",
       "         2,   7,   6,   7,   6,   5,   6,   3,  11,   7,   6,   9,   8,\n",
       "        13,   9,   8,  10,  10,  10,   6,   6,   6,   7,   4,   3,  10,\n",
       "        10,  12,  15,   3,   6,   7,  10,   6,   7,   9,   5,   9,   6,\n",
       "        -2,   2,   3,   3,   6,   5,   3,   2,   3,   4,   3,   7,   6,\n",
       "        11,  14,  10,   8,   7,   8,   6,   9,   2,  -1,   5,   8,   9,\n",
       "         6,  11,  11,   3,   0,  -3,  -3,  -4,  -4,  -4,  -7,  -8,  -7,\n",
       "        -3,  -5,  -3,   0,  -1,   1,  -2,  -2,  -1,  -3,  -1,  -7,  -4,\n",
       "        -2,  -1,   1,   1,  -1,   2,  -2,   4,   1,   0,   2,  -1,   0,\n",
       "        -1,  -1,   1,   2,   1,   0,  -1,   4,   0,   6,   3,   1,   0,\n",
       "         2,   4,   0,   1,   0,  -1,  -1,  -4,   4,  -1,  -3,  -4,  -7,\n",
       "         2,  -3,   1,  -5,  -4,  -1, -11,  -1,  -6,  -3,  -3,  -4,  -1,\n",
       "        -5,   0,   0,   1,   1,  -2,   1,   0,   1,   1,  -3,  -4,  -2,\n",
       "        -3,  -4,  -3,   0,  -4,   2,   0,  -6,   2,   0,  -1,  -1,   2,\n",
       "         2,  -3,  -1,  -4,  -1,   2,   1,  -1,   0,  -2,   0,   3,  -1,\n",
       "         1,   0,   1,   2,   3,   3,   0,  -2,  -3,   1,  -5,  -1,  -3,\n",
       "        -2,  -1,   1,   1,  -2,  -5,   0,   2,   1,   1,  -3,  -1,   1,\n",
       "        -4,  -2,  -6,  -7,  -3,  -4,  -3,  -2,  -6,  -2,  -3,  -2,   0,\n",
       "         0,   2,   1,   1,   0,  -2,  -5,  -4,  -3,  -3,  -4,  -3,  -1,\n",
       "         0,  -1,  -1,  -2,  -1,   7,  -1,   3,  -2,  -2,  -1,   1,  -1,\n",
       "         0,  -2,  -2,  -2,   1,   2,  -1,   0,  -2,  -1,  -2,   1,  -1,\n",
       "        -3,   1,   0,  -2,  -3,   0,  -3,  -3,   2,   0,   1,  -2,   0,\n",
       "        -3,  -1,   1,  -6,   0,  -1])"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict():\n",
    "    y_pred = np.dot(testset.X_t, testset.w).round(0).astype(int)\n",
    "    return y_pred\n",
    "y_pred = predict()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "    'id': ['id_' + str(i) for i in range(500)],\n",
    "    'value': y_pred\n",
    "})\n",
    "pred_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [ 0.21 -0.01  0.    0.03  0.04  0.   -0.02  0.   -0.02 -0.11  0.18  0.05\n",
      " -0.    0.04  0.01  0.21 -0.01 -0.    0.01  0.04 -0.01 -0.02  0.   -0.02\n",
      " -0.08  0.14  0.05 -0.    0.04 -0.01  0.2  -0.01 -0.    0.02  0.04  0.\n",
      " -0.02  0.   -0.02 -0.07  0.1   0.05 -0.    0.04 -0.    0.2  -0.01 -0.01\n",
      "  0.02  0.03 -0.   -0.02  0.   -0.02 -0.06  0.09  0.06 -0.    0.03 -0.\n",
      "  0.2  -0.01 -0.    0.03  0.03 -0.   -0.02  0.   -0.02 -0.05  0.08  0.06\n",
      " -0.    0.03 -0.01  0.2  -0.02 -0.    0.03  0.03  0.   -0.02  0.   -0.02\n",
      " -0.05  0.08  0.06 -0.    0.02 -0.02  0.2  -0.01  0.    0.03  0.03  0.\n",
      " -0.02  0.   -0.02 -0.06  0.1   0.06 -0.    0.02 -0.01  0.21 -0.01  0.\n",
      "  0.03  0.03  0.   -0.02  0.   -0.02 -0.08  0.12  0.06 -0.    0.02 -0.01\n",
      "  0.21 -0.02  0.02  0.05  0.03  0.02 -0.02  0.   -0.02 -0.08  0.18  0.06\n",
      " -0.    0.01 -0.01  0.06  0.03  0.02]\n",
      "w_hat [ 0.83  0.08 -0.1   0.15  3.06 -0.01 -2.17 -2.31  0.03 -0.1   0.15  0.08\n",
      " -0.   -0.15 -0.09  0.27  0.06 -0.06  0.03  1.41 -0.02 -1.15 -0.75 -0.36\n",
      " -0.06  0.11 -0.02 -0.   -0.04 -0.02  0.33  0.04 -0.06  0.05  1.61 -0.\n",
      " -1.11 -0.86 -0.32 -0.05  0.08 -0.16 -0.    0.07  0.01  0.26  0.05 -0.07\n",
      "  0.05  1.24 -0.01 -1.21 -0.7   0.03 -0.04  0.07  0.18 -0.    0.08  0.02\n",
      "  0.35  0.07 -0.09  0.09  1.51 -0.01 -0.93 -1.01 -0.09 -0.03  0.05  0.13\n",
      " -0.   -0.07 -0.01  0.28  0.04 -0.05  0.05  1.03 -0.01 -0.37 -1.81 -0.44\n",
      " -0.03  0.06  0.14 -0.   -0.01 -0.03  0.22  0.03 -0.03  0.04  0.59 -0.01\n",
      " -0.38 -1.54 -0.45 -0.04  0.07  0.06 -0.   -0.23 -0.05  0.34 -0.   -0.01\n",
      "  0.    0.71 -0.01 -0.08 -2.26 -1.1  -0.05  0.09  0.47 -0.    0.25  0.06\n",
      "  0.86  0.04 -0.06  0.08  2.85  0.01 -1.37 -2.83 -0.71 -0.07  0.15  0.85\n",
      " -0.   -0.36 -0.13 -0.01  0.04 71.75]\n",
      "train loss 1836871.9146487094\n",
      "test predict [  3   4  -2   2  -2  -9  10  -5   5  -2   0   0   3   7  -5   1  -5  -3\n",
      "  -3   0  -7  -6   7  -6  -3  -1   0   4   3  -2  -4  -3  -5 -10  -3   1\n",
      "   4  -6  -2  -3  -2   0  -1  -2  -1  -3  17   0  -2   0   4   1   5   2\n",
      "   2   2   2  -3  -4  -3  -1  19  -5  -1   1   1   1   6  12  10   6  12\n",
      "   4   5  11   7   4  -6   0   2  -4   2   2   5   2  12  -6  -2  -3   3\n",
      "   4   4   6   5   3   2   3   2  -9  -2   5   3   0  -1   7  -4  -5  -3\n",
      "  -2   8   7  11  17   6  -2  -3  -5  -5  -9  -8 -13 -10  -9  -4  -7  -8\n",
      "  -7 -16  -6  -4  -2  -7  -7  -7  -3  -1  -8  -6 -14  -8   1   7   4   3\n",
      "  -3  -1   2  -3   3  22  -5  -5  -7  -8 -13 -19  -8  -1  -6  -8 -14 -11\n",
      "  -4  -6   1   3   0 -11 -13 -12 -11  -8  -7  -2  -5  -5  -8 -10  -6  -9\n",
      "  -8 -11 -15  -6  -9  -8   0 -14 -14 -15 -10  -9  -5  -5  -7  -3  -5  -6\n",
      " -12 -11  -1   0  -5  -3   5   0  -1   4   3   4  -7  -7  -3  -9   9   3\n",
      "  -1   0  -2   3   3  -2  -2   0   1   1  -3   9  22   1   1   1   2   0\n",
      "   5   5   5   0   3   6   3   5   6   9   4  26   6   8   2   7  -3   2\n",
      "   2   0   1   1   1   3   6   5  11  13  22  -1   1  -2   0   2  -2  -1\n",
      "   0  -1  -2   1   6  -2   1   0  -6  -6  -6  -6  -8  -3  -1   8   5   2\n",
      "   5  -1   2  -2  -1  -3  -1  -5  -8  -4  -2  -2 -12 -10  -5 -11  -7  -8\n",
      "  -2  -4  -3  -3 -12  -8 -12 -10  -5  -7  -8 -12  -9  -6  -8  -7 -11  -8\n",
      "  -6  -4  -3  -4   1  -2  -2   0  -5   6  -1  -6  -4  -4  -5  -7 -19  -9\n",
      "  -1 -15  -4 -13  -4  -4 -13  -9 -15 -11  -4  -3 -18 -11 -14 -15  -3   3\n",
      "  -4  -4  -4  -8  -4  -4   1  -1  -4   0  -6  -2   0   2  -5   3   5  -8\n",
      "  14 -16 -10 -19  -8 -14 -15 -12 -13   1 -13  -6  -3   0   1  -4   2   3\n",
      "  29  13  24 -38  -8  -5 -15 -21 -19 -23  17   6  10   4   0   6   7   5\n",
      "  16   1   3   3   5   4  -1  -7   2   9   0   0   1  -5   5  -2   3  -7\n",
      " -15 -14  -9  -9 -11  -9  -4  -8  -3   0   0  -2   2   3   7  29  -2   4\n",
      " -16  -5 -17 -11  -9  -5   2  -7  -6  -6   0  -4  -8  -4 -11  -2  -2  -1\n",
      "  -1 -11  -6  -4  -2   1  -4   5   0  -3   1   8 -50 -39   0 -17 -16 -15\n",
      " -18 -16 -12 -10  -6 -10  -7  -6  -9 -11  -9 -14 -12 -14]\n",
      "validation loss 262189.6146360155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soyccan/anaconda3/envs/ml-hw1-tmp/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def closed_form_solution():\n",
    "    w_hat, residuals, rank, s  = np.linalg.lstsq(trainset.X_t, trainset.y_hat)\n",
    "    #print(residuals, rank, s)\n",
    "    X = trainset.X\n",
    "    X_t = trainset.X_t\n",
    "    y_hat = trainset.y_hat\n",
    "    assert w_hat.shape[0] == DIM\n",
    "    print('w',testset.w)\n",
    "    print('w_hat',w_hat)\n",
    "    print('train loss', np.linalg.norm(trainset.X_t.dot(w_hat) - trainset.y_hat)**2)\n",
    "    \n",
    "    print('test predict', np.dot(testset.X_t, w_hat).round(0).astype(int))\n",
    "    \n",
    "    loss = []\n",
    "    d, n = trainset.X.shape\n",
    "    validate_sz = 2000\n",
    "    for i in range(0, n, validate_sz):\n",
    "        idx_validate = pd.Series([False] * n)\n",
    "        idx_validate[i:i+validate_sz] = True\n",
    "        loss.append(validate(X[:, idx_validate], y_hat[idx_validate], testset.w))\n",
    "    print('validation loss', np.mean(loss))\n",
    "\n",
    "closed_form_solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
