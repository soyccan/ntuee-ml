{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "np.set_printoptions(threshold=1e3, suppress=True, precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed(s):\n",
    "    torch.manual_seed(s)\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.X_t = None\n",
    "        self.y_hat = None\n",
    "        self.df = None\n",
    "        self.w = None\n",
    "    def normalize(self):\n",
    "        mean_x = np.nanmean(self.X, axis=1)\n",
    "        std_x = np.nanstd(self.X, axis=1)\n",
    "        std_x[std_x == 0] = 1\n",
    "        self.X = (self.X - mean_x[:, np.newaxis]) / std_x[:, np.newaxis]\n",
    "        self.X_t = self.X.T\n",
    "\n",
    "trainset = Dataset()\n",
    "trainset.full_df0 = pd.read_csv('train_datas_0.csv', dtype='float', na_values='-',\n",
    "                                skiprows=[61-2, 86-2])\n",
    "trainset.full_df1 = pd.read_csv('train_datas_1.csv', dtype='float', na_values='-', \n",
    "                                skiprows=np.arange(2162-2, 2208-2))\n",
    "#.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "testset = Dataset()\n",
    "testset.full_df = pd.read_csv('test_datas.csv', dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['O3', 'WD', 'PM10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_data(sel_cols=['PM2.5']):\n",
    "    trainset.df = pd.concat([trainset.full_df0, trainset.full_df1])\n",
    "    \n",
    "    # add hour and day\n",
    "#     n0 = len(trainset.full_df0)\n",
    "#     n1 = len(trainset.full_df1)\n",
    "#     n = n0 + n1\n",
    "#     trainset.df['hour'] = np.tile(np.arange(24), (n+23)//24)[:n]\n",
    "#     trainset.df['day'] = np.tile(np.repeat(np.arange(365), 24), (n+364)//365)[:n]\n",
    "#     print(trainset.df)\n",
    "\n",
    "    # correct dataframe\n",
    "    trainset.df.dropna(how='all', inplace=True)\n",
    "    trainset.df.drop(trainset.df.index[(trainset.df == 0).all(axis=1)], inplace=True)\n",
    "    for cnam in trainset.df.columns:\n",
    "        # fill NaN with mean\n",
    "        trainset.df[cnam].fillna(trainset.df[cnam].mean(), inplace=True)\n",
    "\n",
    "        # replace outlier with mean\n",
    "        cond = abs(trainset.df[cnam] - trainset.df[cnam].mean()) > 5 * trainset.df[cnam].std()\n",
    "        trainset.df[cnam][cond] = trainset.df[cnam].mean()\n",
    "\n",
    "    trainset.df.reindex()\n",
    "\n",
    "    # extract feature\n",
    "    c = len(sel_cols)\n",
    "    d = c * 9 * 2\n",
    "    n = len(trainset.df) - 9\n",
    "    X = torch.zeros((n, d))\n",
    "    y_hat = torch.zeros((n, 1))\n",
    "    \n",
    "    for i in range(n):\n",
    "        X[i, 0:c*9] = torch.tensor(trainset.df.iloc[i:i+9][sel_cols].values.flatten())\n",
    "        X[i, c*9:c*9*2] = X[i, 0:c*9] ** 2\n",
    "        y_hat[i] = trainset.df.iloc[i+9]['PM2.5']\n",
    "\n",
    "    trainset.X = X\n",
    "    trainset.y_hat = y_hat\n",
    "    #trainset.normalize()\n",
    "\n",
    "preprocess_training_data()#trainset.df.columns.difference(exclude_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(X, y_hat):\n",
    "    n, d = X.shape\n",
    "    print('n,d=',n,d)\n",
    "\n",
    "    #reset_seed(1383267)\n",
    "#     linear_module = torch.nn.Linear(d, 1, bias=True)\n",
    "#     loss_func = torch.nn.MSELoss()\n",
    "#     optim = torch.optim.SGD(linear_module.parameters(), lr=1e-7)#, betas=(0.99, 0.999))\n",
    "\n",
    "#     net = Net(n_feature=d, n_hidden=1, n_output=1) #you can use different n_hidden & lr for test\n",
    "#     print(net)\n",
    "    net = torch.nn.Linear(d, 1, bias=True)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "#     plt.ion()\n",
    "\n",
    "    num_iter = 1000\n",
    "\n",
    "    print('iter,\\tloss,\\tw')\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        y = net(X)\n",
    "        loss = loss_func(y, y_hat) #+ 1e-5 * torch.norm(net.weight)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print('{},\\t{:.2f},\\t{}'.format(i, loss.item(), net.weight.view(d).detach().numpy()))\n",
    "#             print('{},\\t{:.2f}'.format(i, loss.item()))\n",
    "\n",
    "    _X = np.concatenate((X.view((n,d)).detach().numpy(), np.ones((n,1))), axis=1)\n",
    "    _y_hat = y_hat.view(n).detach().numpy()\n",
    "    true_w, residuals, rank, s = np.linalg.lstsq(_X, _y_hat)\n",
    "\n",
    "    print()\n",
    "    print('true w and bias\\t', true_w)\n",
    "    print('true loss\\t', np.linalg.norm(_X @ true_w - _y_hat) / n)\n",
    "    print('estimated w\\t', net.weight.view(d).detach().numpy())\n",
    "    print('estimated bias\\t', net.bias.view(1).detach().numpy())\n",
    "    \n",
    "    testset.net = net\n",
    "    testset.true_w = true_w\n",
    "\n",
    "#train(trainset.X, trainset.y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n,d= 15403 18\n",
      "iter,\tloss,\tw\n",
      "0,\t30832.97,\t[-0.06  0.07 -0.11  0.2   0.   -0.08  0.22 -0.08  0.21 -0.08  0.12  0.17 -0.08\n",
      " -0.17  0.    0.15 -0.11  0.13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soyccan/anaconda3/envs/ml-hw1/lib/python3.6/site-packages/ipykernel_launcher.py:34: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "true w and bias\t [ 0.07  0.    0.03 -0.01  0.02  0.07 -0.01  0.21  0.6  -0.    0.   -0.    0.\n",
      " -0.   -0.    0.   -0.    0.    0.67]\n",
      "true loss\t 0.04351952073365476\n",
      "estimated w\t [ 0.06  0.07 -0.15  0.31  0.17  0.    0.2   0.06  0.22 -0.   -0.    0.   -0.\n",
      " -0.   -0.   -0.   -0.    0.01]\n",
      "estimated bias\t [0.02]\n",
      "validate loss with estimated 98.5306396484375\n",
      "validate loss with true 22.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def validate():\n",
    "    X = trainset.X\n",
    "    y_hat = trainset.y_hat\n",
    "    n, d = X.shape\n",
    "\n",
    "    validate_sz = 2000\n",
    "    losses = []\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    for i in range(0, n, validate_sz):\n",
    "        idx_validate = pd.Series([False] * n)\n",
    "        idx_validate[i:i+validate_sz] = True\n",
    "\n",
    "        train(X[~idx_validate], y_hat[~idx_validate])\n",
    "        y = testset.net(X[idx_validate])\n",
    "        l = loss_func(y, y_hat[idx_validate])\n",
    "        losses.append(l.item())\n",
    "        \n",
    "        break\n",
    "    \n",
    "    vn = validate_sz\n",
    "    _X = np.concatenate((X[idx_validate].numpy(), np.ones((vn, 1))), axis=1)\n",
    "    _y_hat = y_hat[idx_validate].numpy()\n",
    "    true_loss = np.linalg.norm(_X @ testset.true_w - _y_hat) // vn\n",
    "\n",
    "    print('validate loss with estimated', np.mean(losses))\n",
    "    print('validate loss with true', true_loss)\n",
    "    print()\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_testing_data(sel_cols=['PM2.5']):\n",
    "    testset.df = testset.full_df.copy()\n",
    "\n",
    "    c = len(sel_cols)\n",
    "    d = c * 9 * 2\n",
    "    n = (len(testset.df) + 8) // 9\n",
    "    X = torch.zeros((n, d))\n",
    "\n",
    "    # extract feature\n",
    "    for i in range(n):\n",
    "        X[i, 0:c*9] = torch.tensor(testset.df.iloc[i*9:(i+1)*9][sel_cols].values.flatten())\n",
    "        X[i, c*9:c*9*2] = X[i, 0:c*9] ** 2\n",
    "\n",
    "    testset.X = X\n",
    "    #testset.normalize()\n",
    "\n",
    "preprocess_testing_data()#testset.df.columns.difference(exclude_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13. 17. 20. 35. 24. 40. 55. 63. 65. 56. 43. 36. 57. 50. 40. 47. 28. 30. 21.\n",
      " 32. 31. 33. 27. 21. 24. 15.  6.  9. 21. 20. 22. 18. 18. 18. 23. 27. 16. 19.\n",
      "  8.  6. 11. 22. 19. 25. 20. 27. 27. 34. 33. 30. 45. 31. 35. 37. 22. 26. 14.\n",
      " 13.  6.  4.  4.  2.  5.  8.  9.  7.  4.  8. 21. 21. 26. 29. 24. 23. 52. 26.\n",
      " 19. 14. 17. 16. 19. 14.  5. 11. 19. 17. 12.  8.  9. 10. 13. 12. 14. 19. 26.\n",
      " 26. 39. 34. 30. 25. 47. 32. 24. 11.  3. 29. 27. 18. 15.  6.  3. 10. 16. 15.\n",
      " 13. 19. 23. 22. 23. 16. 18. 23. 25. 30. 22. 27. 22. 23. 39. 54. 37. 13. 14.\n",
      "  9. 23. 18. 16. 18. 28. 19. 27. 29. 26. 11. 12. 16. 21. 24. 26. 14.  4.  6.\n",
      "  9. 20. 22. 25. 10. 15. 25. 21. 23. 19. 12. 10.  7. 13. 27. 31. 30. 27. 37.\n",
      " 18.  4. 14. 15. 19. 22. 20. 29. 27. 22. 27. 26. 38. 28. 26. 21. 27. 31. 33.\n",
      " 23. 24. 23. 17. 21. 24. 20. 25. 23. 25. 30. 24. 22. 22. 18. 12. 10.  5.  3.\n",
      " 16. 15. 17. 15. 17. 20. 24. 15. 17. 20. 26. 24. 27. 22. 15. 20. 21. 24. 16.\n",
      "  4. 20. 14. 17. 26. 24. 24. 20. 28. 38. 26. 28. 32. 19.  8. 14. 15. 27. 33.\n",
      " 26. 47. 42. 30. 27. 40. 26. 26. 30. 34. 24. 26. 18. 17. 23.  6. 28. 33. 22.\n",
      " 12. 15. 14. 21. 18. 24. 21. 38. 34. 34. 24. 23. 26. 37. 32. 29. 33. 34. 39.\n",
      " 36. 26. 37. 21. 20. 20. 16. 18.  8. 11. 14. 11. 14. 12.  8.  9.  8.  9. 13.\n",
      " 12.  9.  7.  9.  7.  5.  8.  4. 11. 20. 22. 15. 13. 24. 23. 23. 13. 17. 16.\n",
      " 19. 11.  5.  6.  6.  8.  9.  6.  6.  7.  8. 14. 12. 13.  7. 10. 10.  7. 11.\n",
      " 10. 11. 11. 10.  9. 11.  8.  4.  9.  7.  7. 11.  6. 16.  9.  8.  9.  4.  6.\n",
      "  6.  9.  4.  8.  5.  9.  9.  7.  5.  5.  9.  4. 10.  5.  6.  5.  3.  6.  6.\n",
      "  6.  3. 14.  7. 11. 14.  9.  5.  9. 12.  8.  5.  5.  5.  5.  7.  7.  8. 10.\n",
      "  9.  9. 12.  7. 12.  9.  7.  8.  9. 20.  7.  5.  6. 10.  9.  6. 15.  5.  6.\n",
      "  6.  6.  8.  7.  8.  8.  8. 10. 10. 12.  5.  8.  4.  6.  8.  7.  8.  9.  9.\n",
      "  9.  8.  7.  8.  9. 10. 14.  7. 10.  7.  6.  8.  6. 10.  6.  9.  4.  6.  8.\n",
      " 10.  9.  7. 12. 12. 10. 11.  8. 10.  3. 14.  6.  6.  9.  4.  4. 11.  9.  3.\n",
      "  4.  7.  5.  6.  6.  9. 10. 13. 12.  7.  8.  9.  4.  4.  5. 10.  5.  4.  7.\n",
      "  7.  9.  6.  6. 12. 12.]\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    n, d = testset.X.shape\n",
    "\n",
    "#     y = testset.net(testset.X)\n",
    "#     y = torch.round(y).view(n).detach().numpy()\n",
    "    \n",
    "    _X = np.concatenate((testset.X.numpy(), np.ones((n, 1))), axis=1)\n",
    "    y = _X @ testset.true_w\n",
    "    y = np.rint(y)\n",
    "\n",
    "    testset.y = y\n",
    "    print(y)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "    'id': ['id_' + str(i) for i in range(500)],\n",
    "    'value': testset.y\n",
    "})\n",
    "pred_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
